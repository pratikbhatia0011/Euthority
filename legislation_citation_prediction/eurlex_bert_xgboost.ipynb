{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eurlex_bert_xgboost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "801bcb391c2b495cbbc0ee74f0b8fad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_392928915a774dc19353700021bd5fc9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_22ade9b561de495e8a130346f3e8bb05",
              "IPY_MODEL_375dbfa38cdc4c8a998d03595fb62271"
            ]
          }
        },
        "392928915a774dc19353700021bd5fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22ade9b561de495e8a130346f3e8bb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f1e7b7fa8eb94138a905774d1fc82da2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9c0ce598fd74483284d2fd72b555166a"
          }
        },
        "375dbfa38cdc4c8a998d03595fb62271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_191259e0a93b4f2194bb6175e71d9ea3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.00k/1.00k [00:00&lt;00:00, 2.04kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_169bb3c205b94bc89998fa83f7202a66"
          }
        },
        "f1e7b7fa8eb94138a905774d1fc82da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9c0ce598fd74483284d2fd72b555166a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "191259e0a93b4f2194bb6175e71d9ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "169bb3c205b94bc89998fa83f7202a66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98438b115d264f879819f74882419084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6c2d19c2ca10418aa151b35c0191d666",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_49d955a6f40a44b9ad1624baafb38045",
              "IPY_MODEL_5f508d12933747d6a163544e70f93116"
            ]
          }
        },
        "6c2d19c2ca10418aa151b35c0191d666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49d955a6f40a44b9ad1624baafb38045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ebfc1243cc5c48d5b267d0e5207624ba",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51f80a1546094532ac369a87873763f7"
          }
        },
        "5f508d12933747d6a163544e70f93116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a7c1f02c31044765825e602241bfc367",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 207kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db444007b1ca48cb84ac82dcfda3f9fe"
          }
        },
        "ebfc1243cc5c48d5b267d0e5207624ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51f80a1546094532ac369a87873763f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a7c1f02c31044765825e602241bfc367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db444007b1ca48cb84ac82dcfda3f9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05ced477e667427b81eb3c0a1ddebeb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bc33e7990e6347d3b440604793e2f296",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cfa50152037642f8a9b545c4bc16e8ca",
              "IPY_MODEL_cff62b803a5248cdad5cccce525e7fd4"
            ]
          }
        },
        "bc33e7990e6347d3b440604793e2f296": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cfa50152037642f8a9b545c4bc16e8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_38db8fc63ea34d1492f61d6c894618f2",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440474526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440474526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e56339d791f47dc9b8fbc25044d4701"
          }
        },
        "cff62b803a5248cdad5cccce525e7fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_82b2c73061954f429d1d1c88e72c4501",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:11&lt;00:00, 37.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7f6c71719c94a59920a285e7d690d3f"
          }
        },
        "38db8fc63ea34d1492f61d6c894618f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e56339d791f47dc9b8fbc25044d4701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82b2c73061954f429d1d1c88e72c4501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7f6c71719c94a59920a285e7d690d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkgI0Iwr-ik5",
        "outputId": "d15b30ec-8209-40c7-d25c-3bb475422bd0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.5MB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 27.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9MB 18.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=41651874dc22d2f94f8952efd3a91da52b5f74f364749ac6c257b46927b49a97\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZiZ8VVUBaFN"
      },
      "source": [
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/nlpaueb/legal-bert-base-uncased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIyvR0LP-ktW"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpLQpMd-OUvC",
        "outputId": "ed7cfb6b-4773-4e4e-bd63-fe7b407a6ccd"
      },
      "source": [
        "!git lfs install\r\n",
        "\r\n",
        "!git clone https://huggingface.co/nlpaueb/bert-base-uncased-eurlex\r\n",
        "# !git clone https://huggingface.co/nlpaueb/legal-bert-base-uncased\r\n",
        "\r\n",
        "# if you want to clone without large files â€“ just their pointers\r\n",
        "# prepend your git clone with the following env var:\r\n",
        "GIT_LFS_SKIP_SMUDGE=1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "git: 'lfs' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tlog\n",
            "Cloning into 'bert-base-uncased-eurlex'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 15 (delta 3), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoPgzXOICNyc",
        "outputId": "89e79b11-a274-472c-9155-18a5dd6ffcda"
      },
      "source": [
        "# specify GPU\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ePtaIq9XgG3",
        "outputId": "191cdb44-522b-40cf-e5db-03c6bc5641de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOSHgKyR2BlH"
      },
      "source": [
        "df_cite = pd.read_excel('drive/MyDrive/Data/Euthority/df_cite.xlsx')\r\n",
        "df_cite.rename(columns = {'CELEX_ID': 'celex_num'}, inplace=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFYcpaz9_Yz6",
        "outputId": "19dcc50e-9588-4115-afeb-ee2dec41d38d"
      },
      "source": [
        "dff = pd.read_excel('drive/MyDrive/Data/Euthority/eng_leg_text.xlsx', index_col=0)\n",
        "print(dff.columns)\n",
        "dff['text_size'] = dff.text.apply(lambda x : len(x.split()))\n",
        "# print(df.head())\n",
        "\n",
        "df = pd.merge(dff, df_cite, on='celex_num', how='inner')\n",
        "df['year'] = pd.to_datetime(df.celex_num.apply(lambda x : x[1:5])).dt.year"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['celex_num', 'text', 'text_size', 'descriptor', 'short_text',\n",
            "       'language'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZWQxmyd44hu",
        "outputId": "d6f9c5ad-6d1d-4474-b8d6-64b421623011"
      },
      "source": [
        "df0 = df[(df['cited'] == 0) & (df['text_size'] > 200)].sample(frac=0.5, random_state=2018)\r\n",
        "df1 = df[df['cited'] == 1]\r\n",
        "print(df0.shape, df1.shape)\r\n",
        "print(df1.text_size.describe())\r\n",
        "print(df0.text_size.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(36008, 8) (5170, 8)\n",
            "count    5170.000000\n",
            "mean     1095.793617\n",
            "std      1428.502146\n",
            "min         3.000000\n",
            "25%       245.000000\n",
            "50%       491.500000\n",
            "75%      1181.500000\n",
            "max      6427.000000\n",
            "Name: text_size, dtype: float64\n",
            "count    36008.000000\n",
            "mean       647.568957\n",
            "std        894.719110\n",
            "min        201.000000\n",
            "25%        258.000000\n",
            "50%        366.000000\n",
            "75%        582.000000\n",
            "max       6431.000000\n",
            "Name: text_size, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKkdORmW80El",
        "outputId": "159a4bd0-57c8-4599-c468-3f3bfd4c13e6"
      },
      "source": [
        "df1_tr = pd.DataFrame()\r\n",
        "df1_te = pd.DataFrame()\r\n",
        "for i in df1.year.unique():\r\n",
        "  a = df1[df1['year'] == i]\r\n",
        "  b = a.sample(frac=0.7, random_state=2018)\r\n",
        "  c = a[~a.celex_num.isin(b.celex_num)]\r\n",
        "  df1_tr = df1_tr.append(b)\r\n",
        "  df1_te = df1_te.append(c)\r\n",
        "print(df1_tr.shape, df1_te.shape)\r\n",
        "\r\n",
        "df1_common = pd.merge(df1_tr, df1_te, on='celex_num', how='inner')\r\n",
        "print(df1_common.shape)\r\n",
        "del df1_common\r\n",
        "\r\n",
        "df0_tr, df0_te = pd.DataFrame(), pd.DataFrame()\r\n",
        "for i in df0.year.unique():\r\n",
        "  a = df0[df0['year'] == i]\r\n",
        "  b = a.sample(frac=0.7, random_state=2018)\r\n",
        "  c = a[~a.celex_num.isin(b.celex_num)]\r\n",
        "  df0_tr = df0_tr.append(b)\r\n",
        "  df0_te = df0_te.append(c)\r\n",
        "print(df0_tr.shape, df0_te.shape)\r\n",
        "\r\n",
        "\r\n",
        "df0_common = pd.merge(df0_tr, df0_te, on='celex_num', how='inner')\r\n",
        "print(df0_common.shape)\r\n",
        "del df0_common\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3623, 8) (1547, 8)\n",
            "(0, 15)\n",
            "(25205, 8) (10803, 8)\n",
            "(0, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIKbF5M27RKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a4f15b-7e0c-4247-cad9-ba1058d358c9"
      },
      "source": [
        "# train_set\r\n",
        "# oversampling the imbalanced class in the train set but not in the test set\r\n",
        "df_train = pd.DataFrame()\r\n",
        "df_train = df_train.append(df0_tr)\r\n",
        "for i in range(2):\r\n",
        "  df_train = df_train.append(df1_tr)\r\n",
        "\r\n",
        "df_train = df_train.sample(frac=1, random_state=2018)\r\n",
        "del df0_tr, df1_tr\r\n",
        "# test set\r\n",
        "df_test = pd.DataFrame()\r\n",
        "df_test = df_test.append(df0_te)\r\n",
        "df_test = df_test.append(df1_te)\r\n",
        "df_test = df_test.sample(frac=1, random_state = 2018)\r\n",
        "del df0_te, df1_te\r\n",
        "\r\n",
        "df_train.reset_index(inplace=True)\r\n",
        "df_test.reset_index(inplace=True)\r\n",
        "print(df_train.shape, df_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32451, 9) (12350, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2G01wn1_vlh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "801bcb391c2b495cbbc0ee74f0b8fad3",
            "392928915a774dc19353700021bd5fc9",
            "22ade9b561de495e8a130346f3e8bb05",
            "375dbfa38cdc4c8a998d03595fb62271",
            "f1e7b7fa8eb94138a905774d1fc82da2",
            "9c0ce598fd74483284d2fd72b555166a",
            "191259e0a93b4f2194bb6175e71d9ea3",
            "169bb3c205b94bc89998fa83f7202a66",
            "98438b115d264f879819f74882419084",
            "6c2d19c2ca10418aa151b35c0191d666",
            "49d955a6f40a44b9ad1624baafb38045",
            "5f508d12933747d6a163544e70f93116",
            "ebfc1243cc5c48d5b267d0e5207624ba",
            "51f80a1546094532ac369a87873763f7",
            "a7c1f02c31044765825e602241bfc367",
            "db444007b1ca48cb84ac82dcfda3f9fe",
            "05ced477e667427b81eb3c0a1ddebeb9",
            "bc33e7990e6347d3b440604793e2f296",
            "cfa50152037642f8a9b545c4bc16e8ca",
            "cff62b803a5248cdad5cccce525e7fd4",
            "38db8fc63ea34d1492f61d6c894618f2",
            "7e56339d791f47dc9b8fbc25044d4701",
            "82b2c73061954f429d1d1c88e72c4501",
            "e7f6c71719c94a59920a285e7d690d3f"
          ]
        },
        "outputId": "c890dc3b-7dac-418c-a29b-7a5d7aa0b6c0"
      },
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-uncased-eurlex\", do_lower_case=True)\n",
        "\n",
        "model = AutoModel.from_pretrained(\"nlpaueb/bert-base-uncased-eurlex\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "801bcb391c2b495cbbc0ee74f0b8fad3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1000.0, style=ProgressStyle(descriptionâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98438b115d264f879819f74882419084",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05ced477e667427b81eb3c0a1ddebeb9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440474526.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ8GJC7tBWea"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "    self.data = dataframe\n",
        "    self.text = self.data.text\n",
        "    self.targets = self.data.cited\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.text[idx])\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        pad_to_max_length=True,\n",
        "        truncation = True,\n",
        "        max_length = self.max_len,\n",
        "        return_token_type_ids=False\n",
        "    )\n",
        "\n",
        "    ids = inputs['input_ids']\n",
        "    mask = inputs['attention_mask']\n",
        "    # token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.tensor(ids, dtype=torch.long, device=device),\n",
        "        'attention_mask': torch.tensor(mask, dtype=torch.long, device=device),\n",
        "        # 'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long, device=device),\n",
        "        'label': torch.tensor(self.targets[idx], dtype=torch.long, device=device)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW83T--Bp64S"
      },
      "source": [
        "max_len=480\n",
        "train_dataset = BERTDataset(df_train, tokenizer, max_len)\n",
        "test_dataset = BERTDataset(df_test, tokenizer, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNeMN1e_ttAz"
      },
      "source": [
        "train_batch_size, test_batch_size = 8,4\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size = train_batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size = test_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15mqHnpbMc9R"
      },
      "source": [
        "# for param in model.parameters():\r\n",
        "#   param.requires_grad = False\r\n",
        "\r\n",
        "class BERT_Arch(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, bert):\r\n",
        "      \r\n",
        "      super(BERT_Arch, self).__init__()\r\n",
        "\r\n",
        "      self.bert = bert \r\n",
        "      \r\n",
        "      # dropout layer\r\n",
        "      # self.dropout = nn.Dropout(0.1)\r\n",
        "      \r\n",
        "      # # relu activation function\r\n",
        "      # self.relu =  nn.ReLU()\r\n",
        "\r\n",
        "      # # dense layer 1\r\n",
        "      self.fc1 = nn.Linear(768,2)\r\n",
        "      \r\n",
        "      # # dense layer 2 (Output layer)\r\n",
        "      # self.fc2 = nn.Linear(512,2)\r\n",
        "\r\n",
        "      # #softmax activation function\r\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    #define the forward pass\r\n",
        "    def forward(self, sent_id, mask):\r\n",
        "      # print(sent_id)\r\n",
        "      #pass the inputs to the model  \r\n",
        "      out = self.bert(sent_id, mask)\r\n",
        "      # print(cls_hs)\r\n",
        "      x = self.fc1(out.pooler_output)\r\n",
        "      # # apply softmax activation\r\n",
        "      x = self.softmax(x)\r\n",
        "      y = out.pooler_output\r\n",
        "      return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yeKb3gkNLnJ"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\r\n",
        "bert_model = BERT_Arch(model)\r\n",
        "\r\n",
        "# push the model to GPU\r\n",
        "bert_model = bert_model.to(device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK5aYMYZNZ0L",
        "outputId": "33f5d523-8b5b-449b-8812-a1c53d96a9f5"
      },
      "source": [
        "# define the optimizer\r\n",
        "optimizer = AdamW(bert_model.parameters(), lr = 3e-3)\r\n",
        "\r\n",
        "from sklearn.utils.class_weight import compute_class_weight\r\n",
        "\r\n",
        "# #compute the class weights\r\n",
        "class_wts = compute_class_weight('balanced', np.unique(df_train.cited.tolist()), df_train.cited.tolist())\r\n",
        "\r\n",
        "print(class_wts)\r\n",
        "# convert class weights to tensor\r\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\r\n",
        "weights = weights.to(device)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.64374132 2.23923544]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlpJRzeW1qmC"
      },
      "source": [
        "# loss function\r\n",
        "def loss_fn(outputs, targets, weights):\r\n",
        "    # print('outputs: ', outputs, outputs.size())\r\n",
        "    # print('targets: ', targets, targets.size())\r\n",
        "    # return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))    \r\n",
        "    cross_entropy = nn.NLLLoss(weight=weights)\r\n",
        "    return cross_entropy(outputs, targets)\r\n",
        "# number of training epochs\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ26rNNfvYKK"
      },
      "source": [
        "def train_loop_fn(dataloader, model, optimizer, device, weights, scaler):\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    \n",
        "    # empty list to save model predictions\n",
        "    total_preds, train_targets=[], []\n",
        " \n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # print(len(batch))\n",
        "        # progress update after every 50 batches.\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "        \n",
        "        # push the batch to gpu\n",
        "        batch = [batch[str(r)].to(device) for r in batch]\n",
        "        input_id, mask, targets = batch\n",
        "      \n",
        "        optimizer.zero_grad()\n",
        "        with amp.autocast():\n",
        "            out_x, out_y = model(input_id, mask)\n",
        "            \n",
        "            loss = loss_fn(out_x, targets, weights)\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # optimizer.step()\n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        total_preds.extend(out_y.detach().cpu().numpy())\n",
        "        train_targets.extend(targets.detach().cpu().numpy())\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    # avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    # total_preds  = np.concatenate(total_preds, axis=0)\n",
        "    xgb_df_train = pd.DataFrame(total_preds)\n",
        "    \n",
        "    #returns the loss and predictions\n",
        "    return xgb_df_train, train_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ntxqYQkXpWK"
      },
      "source": [
        "def eval_fn(eval_dataloader, model, device):\n",
        "    model.eval()\n",
        "    preds, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(eval_dataloader):\n",
        "\n",
        "            # push the batch to gpu\n",
        "            batch = [batch[str(r)].to(device) for r in batch]\n",
        "            input_id, mask, targets = batch\n",
        "\n",
        "            out_x, out_y = model(input_id, mask)\n",
        "\n",
        "            true_labels.extend(targets.detach().cpu().numpy())\n",
        "            preds.extend(out_y.detach().cpu().numpy())\n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    # preds  = np.concatenate(preds, axis=0)\n",
        "    xgb_df_test = pd.DataFrame(preds)\n",
        "\n",
        "    return xgb_df_test, true_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHiqLNWhzhGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c212a7e0-60a1-4475-df7c-4b9b8e0b2567"
      },
      "source": [
        "epochs = 3\n",
        "# num_training_steps = int(len(df_train) / (batch_size * epochs))\n",
        "# print(num_training_steps)\n",
        "scaler = amp.GradScaler()\n",
        "for epoch in range(epochs):\n",
        "    train_df, train_target = train_loop_fn(train_dataloader, bert_model, optimizer, device, weights, scaler)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch   500  of  4,057.\n",
            "  Batch 1,000  of  4,057.\n",
            "  Batch 1,500  of  4,057.\n",
            "  Batch 2,000  of  4,057.\n",
            "  Batch 2,500  of  4,057.\n",
            "  Batch 3,000  of  4,057.\n",
            "  Batch 3,500  of  4,057.\n",
            "  Batch 4,000  of  4,057.\n",
            "  Batch   500  of  4,057.\n",
            "  Batch 1,000  of  4,057.\n",
            "  Batch 1,500  of  4,057.\n",
            "  Batch 2,000  of  4,057.\n",
            "  Batch 2,500  of  4,057.\n",
            "  Batch 3,000  of  4,057.\n",
            "  Batch 3,500  of  4,057.\n",
            "  Batch 4,000  of  4,057.\n",
            "  Batch   500  of  4,057.\n",
            "  Batch 1,000  of  4,057.\n",
            "  Batch 1,500  of  4,057.\n",
            "  Batch 2,000  of  4,057.\n",
            "  Batch 2,500  of  4,057.\n",
            "  Batch 3,000  of  4,057.\n",
            "  Batch 3,500  of  4,057.\n",
            "  Batch 4,000  of  4,057.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC5BghG6Lg2h"
      },
      "source": [
        "print(train_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaKm31rLhGMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985912b1-fcd4-4a2b-8230-415cbf93225b"
      },
      "source": [
        "test_df, test_target = eval_fn(test_dataloader, bert_model, device)\r\n",
        "print(test_df.shape)\r\n",
        "# print(test_target.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2179: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(12350, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fs1sFvmvBcE"
      },
      "source": [
        "# xgb_train_df = pd.DataFrame(train_list)\r\n",
        "# print(xgb_train_df.shape)\r\n",
        "# xgb_train_df = train_df.copy()\r\n",
        "# xgb_train_df['label'] = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcEc2hFvwPm"
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqhe9Sc2v3j7"
      },
      "source": [
        "# preparing dmatrix for xgboost\r\n",
        "d_train = xgb.DMatrix(train_df, label = train_target)\r\n",
        "d_test = xgb.DMatrix(test_df, label = test_target)\r\n",
        "\r\n",
        "# extreme gradient boosting\r\n",
        "model_name = 'XGBoost'\r\n",
        "parameters = {\"max_depth\":15, \"eta\":0.005, \"gamma\":1, \"sampling_method\":\"uniform\", \\\r\n",
        "              \"subsample\":0.5, \"scale_pos_weight\":2, \"objective\":\"binary:logistic\", \\\r\n",
        "              \"verbosity\":0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAaazRnx0SI6"
      },
      "source": [
        "model = xgb.train(parameters, d_train, 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFDc0E9CwaeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40455548-4c05-454d-8345-687dc81a8b29"
      },
      "source": [
        "test_pred = model.predict(d_test)\r\n",
        "print(test_pred)\r\n",
        "y_pred = np.where(test_pred <= 0.5, 0, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.1686817  0.26949108 0.15825115 ... 0.06161566 0.33762386 0.12855875]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykA0ctOVyAit"
      },
      "source": [
        "Increase oversampling and check performance \\\r\n",
        "Try XGBoost after the pooler output \\\r\n",
        "Try better hyper parameters \\\r\n",
        "Try the other loss functions for binary classification\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgO_4g-fo7hQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e3a431-d9cd-4199-c516-cf94ab6cd683"
      },
      "source": [
        "print('F1-score: ', f1_score(test_target, y_pred))\r\n",
        "print(classification_report(test_target, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score:  0.5616272009714631\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.92      0.93     10803\n",
            "           1       0.53      0.60      0.56      1547\n",
            "\n",
            "    accuracy                           0.88     12350\n",
            "   macro avg       0.74      0.76      0.75     12350\n",
            "weighted avg       0.89      0.88      0.89     12350\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHV6X_dTEHAD"
      },
      "source": [
        "trdf = train_df.copy()\r\n",
        "trdf['target'] = train_target\r\n",
        "tedf = test_df.copy()\r\n",
        "tedf['target'] = test_target\r\n",
        "trdf.to_excel('drive/MyDrive/Data/Euthority/eurlex_train_df.xlsx')\r\n",
        "tedf.to_excel('drive/MyDrive/Data/Euthority/eurlex_test_df.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFpflYGhw1vn"
      },
      "source": [
        "# unique_list = []\r\n",
        "# for i in outputs:\r\n",
        "#   if i not in unique_list:\r\n",
        "#     unique_list.append(i)\r\n",
        "#   else:\r\n",
        "#     pass\r\n",
        "# print(unique_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZt_03pWzNfh"
      },
      "source": [
        "\n",
        "# define the optimizer\n",
        "# optimizer = AdamW(bert_model.parameters(),\n",
        "#                   lr = 5e-5)          # learning rate\n",
        "# \n",
        "# param_optimizer = list(model.named_parameters())\n",
        "# no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "# optimizer_parameters = [\n",
        "#     {\n",
        "#         \"params\": [\n",
        "#             p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
        "#         ],\n",
        "#         \"weight_decay\": 0.001,\n",
        "#     },\n",
        "#     {\n",
        "#         \"params\": [\n",
        "#             p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
        "#         ],\n",
        "#         \"weight_decay\": 0.0,\n",
        "#     },\n",
        "# ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxf5PbjWvEfC"
      },
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# #compute the class weights\n",
        "# class_weights = compute_class_weight('balanced', train_df.cited.unique() , train_df.cited)\n",
        "\n",
        "# print(\"Class Weights:\",class_weights)\n",
        "# # converting list of class weights to a tensor\n",
        "# weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# # push to GPU\n",
        "# weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "\n",
        "# def loss_fn(outputs, targets):\n",
        "#     # print('outputs: ', outputs, outputs.size())\n",
        "#     # print('targets: ', targets, targets.size())\n",
        "#     return nn.BCEWithLogitsLoss()(outputs, targets)    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
