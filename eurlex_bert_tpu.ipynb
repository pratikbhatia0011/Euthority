{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eurlex_bert_tpu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkgI0Iwr-ik5",
        "outputId": "a0884c31-5742-4a49-8131-fca890e64dc6"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUB12htcqU9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5f05579-ad37-44f9-9121-6f8f0ddf7f00"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.6/dist-packages (0.10)\n",
            "Requirement already satisfied: torch-xla==1.7 from https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.7)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.6)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.17.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.12.4)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (50.3.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZiZ8VVUBaFN"
      },
      "source": [
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/nlpaueb/legal-bert-base-uncased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIyvR0LP-ktW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "f46b6bf6-d362-435d-d77c-d90bae95374c"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from tqdm import tqdm\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-240379c2fd67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpLQpMd-OUvC",
        "outputId": "b353e62a-ff68-46b7-e38d-597ac3f67968"
      },
      "source": [
        "!git lfs install\r\n",
        "\r\n",
        "!git clone https://huggingface.co/nlpaueb/bert-base-uncased-eurlex\r\n",
        "# !git clone https://huggingface.co/nlpaueb/legal-bert-base-uncased\r\n",
        "\r\n",
        "# if you want to clone without large files â€“ just their pointers\r\n",
        "# prepend your git clone with the following env var:\r\n",
        "GIT_LFS_SKIP_SMUDGE=1 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "git: 'lfs' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tlog\n",
            "fatal: destination path 'bert-base-uncased-eurlex' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoPgzXOICNyc"
      },
      "source": [
        "device = xm.xla_device()\n",
        "# specify GPU\n",
        "# if torch.cuda.is_available():    \n",
        "\n",
        "#     # Tell PyTorch to use the GPU.    \n",
        "#     device = torch.device(\"cuda\")\n",
        "\n",
        "#     print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "#     print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# # If not...\n",
        "# else:\n",
        "#     print('No GPU available, using the CPU instead.')\n",
        "#     device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ePtaIq9XgG3",
        "outputId": "06eaaaff-877f-48d7-9dc8-4ef4a42a56cc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOSHgKyR2BlH"
      },
      "source": [
        "df_cite = pd.read_excel('drive/MyDrive/Data/Euthority/df_cite.xlsx')\r\n",
        "df_cite.rename(columns = {'CELEX_ID': 'celex_num'}, inplace=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFYcpaz9_Yz6",
        "outputId": "7a23152f-fb47-4be6-e72d-256df664c18a"
      },
      "source": [
        "dff = pd.read_excel('drive/MyDrive/Data/Euthority/eng_leg_text.xlsx', index_col=0)\n",
        "print(dff.columns)\n",
        "dff['text_size'] = dff.text.apply(lambda x : len(x.split()))\n",
        "# print(df.head())\n",
        "\n",
        "df = pd.merge(dff, df_cite, on='celex_num', how='inner')\n",
        "df['year'] = pd.to_datetime(df.celex_num.apply(lambda x : x[1:5])).dt.year"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['celex_num', 'text', 'text_size', 'descriptor', 'short_text',\n",
            "       'language'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZWQxmyd44hu",
        "outputId": "397059e7-57f5-4d55-e9b5-4263a9807dde"
      },
      "source": [
        "df0 = df[(df['cited'] == 0) & (df['text_size'] > 200)].sample(frac=0.5, random_state=2018)\r\n",
        "df1 = df[df['cited'] == 1]\r\n",
        "print(df0.shape, df1.shape)\r\n",
        "print(df1.text_size.describe())\r\n",
        "print(df0.text_size.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(36008, 8) (5170, 8)\n",
            "count    5170.000000\n",
            "mean     1095.793617\n",
            "std      1428.502146\n",
            "min         3.000000\n",
            "25%       245.000000\n",
            "50%       491.500000\n",
            "75%      1181.500000\n",
            "max      6427.000000\n",
            "Name: text_size, dtype: float64\n",
            "count    36008.000000\n",
            "mean       647.568957\n",
            "std        894.719110\n",
            "min        201.000000\n",
            "25%        258.000000\n",
            "50%        366.000000\n",
            "75%        582.000000\n",
            "max       6431.000000\n",
            "Name: text_size, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKkdORmW80El",
        "outputId": "380263df-dd4f-4134-c670-539d8a37687b"
      },
      "source": [
        "df1_tr = pd.DataFrame()\r\n",
        "df1_te = pd.DataFrame()\r\n",
        "for i in df1.year.unique():\r\n",
        "  a = df1[df1['year'] == i]\r\n",
        "  b = a.sample(frac=0.7, random_state=2018)\r\n",
        "  c = a[~a.celex_num.isin(b.celex_num)]\r\n",
        "  df1_tr = df1_tr.append(b)\r\n",
        "  df1_te = df1_te.append(c)\r\n",
        "print(df1_tr.shape, df1_te.shape)\r\n",
        "\r\n",
        "df1_common = pd.merge(df1_tr, df1_te, on='celex_num', how='inner')\r\n",
        "print(df1_common.shape)\r\n",
        "del df1_common\r\n",
        "\r\n",
        "df0_tr, df0_te = pd.DataFrame(), pd.DataFrame()\r\n",
        "for i in df0.year.unique():\r\n",
        "  a = df0[df0['year'] == i]\r\n",
        "  b = a.sample(frac=0.7, random_state=2018)\r\n",
        "  c = a[~a.celex_num.isin(b.celex_num)]\r\n",
        "  df0_tr = df0_tr.append(b)\r\n",
        "  df0_te = df0_te.append(c)\r\n",
        "print(df0_tr.shape, df0_te.shape)\r\n",
        "\r\n",
        "\r\n",
        "df0_common = pd.merge(df0_tr, df0_te, on='celex_num', how='inner')\r\n",
        "print(df0_common.shape)\r\n",
        "del df0_common\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3623, 8) (1547, 8)\n",
            "(0, 15)\n",
            "(25205, 8) (10803, 8)\n",
            "(0, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIKbF5M27RKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9cfddc8-6f14-49a6-946b-3c4f6e7808a4"
      },
      "source": [
        "# train_set\r\n",
        "# oversampling the imbalanced class in the train set but not in the test set\r\n",
        "df_train = pd.DataFrame()\r\n",
        "df_train = df_train.append(df0_tr)\r\n",
        "for i in range(3):\r\n",
        "  df_train = df_train.append(df1_tr)\r\n",
        "\r\n",
        "df_train = df_train.sample(frac=1, random_state=2018)\r\n",
        "del df0_tr, df1_tr\r\n",
        "# test set\r\n",
        "df_test = pd.DataFrame()\r\n",
        "df_test = df_test.append(df0_te)\r\n",
        "df_test = df_test.append(df1_te)\r\n",
        "df_test = df_test.sample(frac=1, random_state = 2018)\r\n",
        "del df0_te, df1_te\r\n",
        "\r\n",
        "df_train.reset_index(inplace=True)\r\n",
        "df_test.reset_index(inplace=True)\r\n",
        "print(df_train.shape, df_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32451, 9) (12350, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2G01wn1_vlh"
      },
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-uncased-eurlex\", do_lower_case=True)\n",
        "\n",
        "model = AutoModel.from_pretrained(\"nlpaueb/bert-base-uncased-eurlex\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ8GJC7tBWea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "20bf87de-4a4c-4b30-fa37-3fde5ed8e099"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, dataframe, tokenizer, max_len):\n",
        "    self.data = dataframe\n",
        "    self.text = self.data.text\n",
        "    self.targets = self.data.cited\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.text[idx])\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        pad_to_max_length=True,\n",
        "        truncation = True,\n",
        "        max_length = self.max_len,\n",
        "        return_token_type_ids=False\n",
        "    )\n",
        "\n",
        "    ids = inputs['input_ids']\n",
        "    mask = inputs['attention_mask']\n",
        "    # token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.tensor(ids, dtype=torch.long, device=device),\n",
        "        'attention_mask': torch.tensor(mask, dtype=torch.long, device=device),\n",
        "        # 'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long, device=device),\n",
        "        'label': torch.tensor(self.targets[idx], dtype=torch.long, device=device)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-14a4f5f8aa66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBERTDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW83T--Bp64S"
      },
      "source": [
        "max_len=480\n",
        "train_dataset = BERTDataset(df_train, tokenizer, max_len)\n",
        "test_dataset = BERTDataset(df_test, tokenizer, max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNeMN1e_ttAz"
      },
      "source": [
        "train_batch_size, test_batch_size = 8,4\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size = train_batch_size, drop_last=True)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size = test_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15mqHnpbMc9R"
      },
      "source": [
        "# for param in model.parameters():\r\n",
        "#   param.requires_grad = False\r\n",
        "\r\n",
        "class BERT_Arch(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, bert):\r\n",
        "      \r\n",
        "      super(BERT_Arch, self).__init__()\r\n",
        "\r\n",
        "      self.bert = bert \r\n",
        "      \r\n",
        "      # dropout layer\r\n",
        "      self.dropout = nn.Dropout(0.1)\r\n",
        "      \r\n",
        "      # relu activation function\r\n",
        "      self.relu =  nn.ReLU()\r\n",
        "\r\n",
        "      # # dense layer 1\r\n",
        "      self.fc1 = nn.Linear(768,2)\r\n",
        "      \r\n",
        "      # dense layer 2 (Output layer)\r\n",
        "      self.fc2 = nn.Linear(512,2)\r\n",
        "\r\n",
        "      #softmax activation function\r\n",
        "      self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    #define the forward pass\r\n",
        "    def forward(self, sent_id, mask):\r\n",
        "      # print(sent_id)\r\n",
        "      #pass the inputs to the model  \r\n",
        "      out = self.bert(sent_id, mask)\r\n",
        "      # print(cls_hs)\r\n",
        "      x = self.fc1(out.pooler_output)\r\n",
        "\r\n",
        "      x = self.dropout(x)\r\n",
        "\r\n",
        "      # output layer\r\n",
        "      x = self.fc2(x)\r\n",
        "      \r\n",
        "      # apply softmax activation\r\n",
        "      x = self.softmax(x)\r\n",
        "      \r\n",
        "      y = out.pooler_output\r\n",
        "      \r\n",
        "      return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yeKb3gkNLnJ"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\r\n",
        "bert_model = BERT_Arch(model)\r\n",
        "\r\n",
        "# push the model to GPU\r\n",
        "bert_model = bert_model.to(device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK5aYMYZNZ0L",
        "outputId": "4bbc3fce-df25-4121-d33f-6c64888f77c2"
      },
      "source": [
        "# define the optimizer\r\n",
        "optimizer = AdamW(bert_model.parameters(), lr = 3e-4)\r\n",
        "\r\n",
        "from sklearn.utils.class_weight import compute_class_weight\r\n",
        "\r\n",
        "# #compute the class weights\r\n",
        "class_wts = compute_class_weight('balanced', np.unique(df_train.cited.tolist()), df_train.cited.tolist())\r\n",
        "\r\n",
        "print(class_wts)\r\n",
        "# convert class weights to tensor\r\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\r\n",
        "weights = weights.to(device)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.64374132 2.23923544]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlpJRzeW1qmC"
      },
      "source": [
        "# loss function\r\n",
        "def loss_fn(outputs, targets, weights):\r\n",
        "    # print('outputs: ', outputs, outputs.size())\r\n",
        "    # print('targets: ', targets, targets.size())\r\n",
        "    # return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))    \r\n",
        "    cross_entropy = nn.NLLLoss(weight=weights)\r\n",
        "    return cross_entropy(outputs, targets)\r\n",
        "# number of training epochs \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ26rNNfvYKK"
      },
      "source": [
        "def train_loop_fn(dataloader, model, optimizer, device, weights):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    # empty list to save model predictions\n",
        "    total_preds, train_targets=[], []\n",
        " \n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # print(len(batch))\n",
        "        # progress update after every 50 batches.\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "        \n",
        "        # push the batch to gpu\n",
        "        batch = [batch[str(r)].to(device) for r in batch]\n",
        "        input_id, mask, targets = batch\n",
        "      \n",
        "        optimizer.zero_grad()\n",
        "        # with amp.autocast():\n",
        "        out_x, out_y = model(input_id, mask)\n",
        "        \n",
        "        loss = loss_fn(out_x, targets, weights)\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        xm.optimizer_step(optimizer, barrier=True)\n",
        "        \n",
        "        \n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        total_preds.extend(out_y.detach().cpu().numpy())\n",
        "        train_targets.extend(targets.detach().cpu().numpy())\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    # avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    # total_preds  = np.concatenate(total_preds, axis=0)\n",
        "    xgb_df_train = pd.DataFrame(total_preds)\n",
        "    \n",
        "    #returns the loss and predictions\n",
        "    return xgb_df_train, train_targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ntxqYQkXpWK"
      },
      "source": [
        "def eval_fn(eval_dataloader, model, device):\n",
        "    model.eval()\n",
        "    preds, true_labels = [], []\n",
        "    # with torch.no_grad():\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [batch[str(r)].to(device) for r in batch]\n",
        "        input_id, mask, targets = batch\n",
        "\n",
        "        out_x, out_y = model(input_id, mask)\n",
        "\n",
        "        true_labels.extend(targets.detach().cpu().numpy())\n",
        "        preds.extend(out_y.detach().cpu().numpy())\n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    # preds  = np.concatenate(preds, axis=0)\n",
        "    xgb_df_test = pd.DataFrame(preds)\n",
        "\n",
        "    return xgb_df_test, true_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHiqLNWhzhGs"
      },
      "source": [
        "epochs = 3\n",
        "# # num_training_steps = int(len(df_train) / (batch_size * epochs))\n",
        "# # print(num_training_steps)\n",
        "# # scaler = amp.GradScaler()\n",
        "for epoch in range(epochs):\n",
        "    train_df, train_target = train_loop_fn(train_dataloader, bert_model, optimizer, device, weights)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC5BghG6Lg2h"
      },
      "source": [
        "# print(train_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaKm31rLhGMI"
      },
      "source": [
        "# test_df, test_target = eval_fn(test_dataloader, bert_model, device)\r\n",
        "# print(test_df.shape)\r\n",
        "# print(test_target.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fs1sFvmvBcE"
      },
      "source": [
        "# xgb_train_df = pd.DataFrame(train_list)\r\n",
        "# print(xgb_train_df.shape)\r\n",
        "# xgb_train_df = train_df.copy()\r\n",
        "# xgb_train_df['label'] = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcEc2hFvwPm"
      },
      "source": [
        "# import xgboost as xgb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqhe9Sc2v3j7"
      },
      "source": [
        "# preparing dmatrix for xgboost\r\n",
        "# d_train = xgb.DMatrix(train_df, label = train_target)\r\n",
        "# d_test = xgb.DMatrix(test_df, label = test_target)\r\n",
        "\r\n",
        "# extreme gradient boosting\r\n",
        "# model_name = 'XGBoost'\r\n",
        "# parameters = {\"max_depth\":15, \"eta\":0.005, \"gamma\":1, \"sampling_method\":\"uniform\", \\\r\n",
        "#               \"subsample\":0.5, \"scale_pos_weight\":4, \"objective\":\"binary:logistic\", \\\r\n",
        "#               \"verbosity\":0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAaazRnx0SI6"
      },
      "source": [
        "# model = xgb.train(parameters, d_train, 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFDc0E9CwaeK"
      },
      "source": [
        "# test_pred = model.predict(d_test)\r\n",
        "# print(test_pred)\r\n",
        "# y_pred = np.where(test_pred <= 0.5, 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykA0ctOVyAit"
      },
      "source": [
        "Increase oversampling and check performance \\\r\n",
        "Try XGBoost after the pooler output \\\r\n",
        "Try better hyper parameters \\\r\n",
        "Try the other loss functions for binary classification\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgO_4g-fo7hQ"
      },
      "source": [
        "\r\n",
        "# print('F1-score: ', f1_score(test_target, y_pred))\r\n",
        "# print(classification_report(test_target, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}